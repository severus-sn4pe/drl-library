{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f88133-175b-4a0d-877e-a6ed4d2f301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "from tbparse import SummaryReader\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae593e-60aa-4763-9698-ed02e2c22d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logs(run_name, base='../tensorboard_log/cs', pivot=True, extra_columns=None):\n",
    "    f = os.path.join(base, run_name)\n",
    "    return SummaryReader(f, pivot=pivot, extra_columns=extra_columns)\n",
    "\n",
    "def parse_scalars(reader):\n",
    "    \"\"\"\n",
    "    parses the scalar dataframe for a SummaryReader object.\n",
    "    extracts first element from wall_time column and removes prefixes from column names\n",
    "    \"\"\"\n",
    "    scalars = reader.scalars\n",
    "    if 'wall_time' in scalars.columns:\n",
    "        # convert wall time list into single number column, just use first value\n",
    "        scalars['wall_time'] = [x[0] for x in scalars['wall_time'].tolist()]\n",
    "    from_columns = scalars.columns.tolist()\n",
    "    to_columns = [x.replace('my-stats/', '').replace('train/', '').replace('time/', '') for x in from_columns]\n",
    "    scalars = scalars.rename(columns=dict(zip(from_columns, to_columns)))\n",
    "    return scalars\n",
    "\n",
    "def get_filenames(model, run_cfg, base='../tensorboard_log/cs'):\n",
    "    \"\"\"\n",
    "    gets filename base on model and run_cfg filter and sorted them by datestr (3rd element in _.split)\n",
    "    removes invalid names that do not match the filter conditions\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    timestamps = []\n",
    "    for filename in os.listdir(base):\n",
    "        f = os.path.join(base, filename)\n",
    "        if not os.path.isfile(f):\n",
    "            fn_parts = filename.split('_')\n",
    "            if parts_match_filter(fn_parts, model, run_cfg):\n",
    "                items.append(filename)\n",
    "                timestamps.append(int(fn_parts[2]))\n",
    "    sorted_items = [x for _, x in sorted(zip(timestamps, items))]\n",
    "    return sorted_items\n",
    "\n",
    "def parts_match_filter(parts, model, run_cfg):\n",
    "    if len(parts) < 3:\n",
    "        return False\n",
    "    if parts[0] != model:\n",
    "        return False\n",
    "    if parts[1] != run_cfg:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def fix_counters(dataframes):\n",
    "    \"\"\"\n",
    "    Adds max. step / episode from previous df to next to have continuous values\n",
    "    \"\"\"\n",
    "    # correct step and episode counters\n",
    "    start_step = 0\n",
    "    start_ep   = 0\n",
    "    for d in dataframes:\n",
    "        d['step'] += start_step\n",
    "        d['episode'] += start_ep\n",
    "        start_step = d.iloc[-1]['step']\n",
    "        start_ep   = d.iloc[-1]['episode']\n",
    "        d['step'] = d['step'].astype('int')\n",
    "        d['episode'] = d['episode'].astype('int')                                     \n",
    "    return dataframes\n",
    "\n",
    "def load_dataframes(filenames):\n",
    "    \"\"\"\n",
    "    loads a set of tensorboard logs into scalar dataframes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for fn in filenames:\n",
    "        reader = get_logs(fn, extra_columns={'wall_time', })\n",
    "        sc = parse_scalars(reader)\n",
    "        # print(f\"loaded {sc.shape[0]} rows from {fn}\")\n",
    "        data.append(sc)\n",
    "    return data\n",
    "\n",
    "def merge_data(dataframes, add_date_column=False):\n",
    "    df_all = pd.concat(dataframes).reset_index(drop=True)\n",
    "    df_all['wall_time'] = df_all['wall_time'].astype('int')\n",
    "    if add_date_column:\n",
    "        df_all['date'] = pd.to_datetime(df_all['wall_time'], unit='s').astype('datetime64[s]')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8a5d8-3616-445a-b9dc-bb8fb9bc69d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3305f15-b615-480e-af67-5cbfce23fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tensorlogs(model, cfg, base_path='../tensorboard_log/cs', target_path='../logs_parsed', add_date=False):\n",
    "    print(f\"Combine Logs for {model}_{cfg}\")\n",
    "    start = time.time()\n",
    "    filenames = get_filenames(model, cfg)\n",
    "    \n",
    "    if len(filenames) == 0:\n",
    "        print(f\"No files found - skipping\")\n",
    "        return\n",
    "    \n",
    "    x = load_dataframes(filenames)\n",
    "    x = fix_counters(x)\n",
    "    x = merge_data(x, add_date_column=add_date)\n",
    "\n",
    "    x.to_csv(f\"{target_path}/{model}_{cfg}.csv\")\n",
    "    print(f\"Combine Logs for {model}_{cfg} done in {(time.time() - start):.1f}s\")\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d495c-bb02-412a-9e15-243bd5efef6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82908c55-6a2a-4cfc-bc1f-9db48b61dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = ['V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ce81c-2840-4e15-8956-7024dc973097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cf603-2054-432c-bde2-2f4951247d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['A2C']\n",
    "run_configs = ['V208']\n",
    "model = models[0]\n",
    "run_cfg = run_configs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9543930-7123-4184-a689-4f31d4c73c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919d12c-2f07-4df4-a160-30b4f481b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg in cfgs:\n",
    "    df = combine_tensorlogs(\"A2C\", cfg, add_date=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9031b84-404f-4cdd-b993-b31f7a9b0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cfg = 'V210'\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0ae99-a35f-4e2b-97ba-1f7df03a8bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff363ae-144e-4cf1-b926-cc9e138c88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3ba66-c210-4b8c-a940-1f3f197b1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca03c9-2450-4110-a86b-5065b8b3d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_data['date'] = pd.to_datetime(all_data['wall_time'], unit='s').astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2588b7e-5afb-405c-9749-a0ec5f99a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71d6e2-7915-4371-8931-8fae22fb60fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d44e77-a160-4632-a5e3-721556079df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54b829-1c1c-45ba-9a8a-3285520f22be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
