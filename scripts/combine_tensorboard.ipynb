{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59519fcd-c11e-4932-b1ac-4502acb4e49e",
   "metadata": {},
   "source": [
    "# Combine Tensorboard Logs\n",
    "\n",
    "This file extracts all logged tensorboard data from multiple runs of model configurations and writes the combined dataframe into a new file.\n",
    "\n",
    "* adds the step counter / episode from previous runs to all subsequent files to have a continuous increment across files of different runs\n",
    "* can add a datetime variable with `add_date=True`\n",
    "* can remove all non-eval rows of datasets to make statistics file more compact with `keep_only_eval_rows=True`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f88133-175b-4a0d-877e-a6ed4d2f301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "from tbparse import SummaryReader\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ae593e-60aa-4763-9698-ed02e2c22d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logs(run_name, base='../tensorboard_log/cs', pivot=True, extra_columns=None):\n",
    "    f = os.path.join(base, run_name)\n",
    "    return SummaryReader(f, pivot=pivot, extra_columns=extra_columns)\n",
    "\n",
    "def parse_scalars(reader):\n",
    "    \"\"\"\n",
    "    parses the scalar dataframe for a SummaryReader object.\n",
    "    extracts first element from wall_time column and removes prefixes from column names\n",
    "    \"\"\"\n",
    "    scalars = reader.scalars\n",
    "    if 'wall_time' in scalars.columns:\n",
    "        # convert wall time list into single number column, just use first value\n",
    "        scalars['wall_time'] = [x[0] for x in scalars['wall_time'].tolist()]\n",
    "    from_columns = scalars.columns.tolist()\n",
    "    to_columns = [x.replace('my-stats/', '').replace('train/', '').replace('time/', '').replace('eval/', 'eval_') for x in from_columns]\n",
    "    scalars = scalars.rename(columns=dict(zip(from_columns, to_columns)))\n",
    "    return scalars\n",
    "\n",
    "def get_filenames(model, run_cfg, base='../tensorboard_log/cs', res=None):\n",
    "    \"\"\"\n",
    "    gets filename base on model and run_cfg filter and sorted them by datestr (3rd element in _.split)\n",
    "    optional filter res (4th element) for intraday\n",
    "    removes invalid names that do not match the filter conditions\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    timestamps = []\n",
    "    for filename in os.listdir(base):\n",
    "        f = os.path.join(base, filename)\n",
    "        if not os.path.isfile(f):\n",
    "            fn_parts = filename.split('_')\n",
    "            if parts_match_filter(fn_parts, model, run_cfg, res):\n",
    "                items.append(filename)\n",
    "                timestamps.append(int(fn_parts[2]))\n",
    "    sorted_items = [x for _, x in sorted(zip(timestamps, items))]\n",
    "    return sorted_items\n",
    "\n",
    "def parts_match_filter(parts, model, run_cfg, res=None):\n",
    "    if len(parts) < 3:\n",
    "        return False\n",
    "    if parts[0] != model:\n",
    "        return False\n",
    "    if parts[1] != run_cfg:\n",
    "        return False\n",
    "    if res:\n",
    "        if parts[3] != res:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def fix_counters(dataframes):\n",
    "    \"\"\"\n",
    "    Adds max. step / episode from previous df to next to have continuous values\n",
    "    \"\"\"\n",
    "    # correct step and episode counters\n",
    "    start_step = 0\n",
    "    start_ep   = 0\n",
    "    print(f\" - fixing counters of individual datasets\")\n",
    "    for d in dataframes:\n",
    "        d['step'] += start_step\n",
    "        d['episode'] += start_ep\n",
    "        start_step = d.iloc[-1]['step']\n",
    "        start_ep   = d.iloc[-1]['episode']\n",
    "        d['step'] = d['step'].astype('Int64')\n",
    "        d['episode'] = d['episode'].astype('Int64')                                     \n",
    "    return dataframes\n",
    "\n",
    "def load_dataframes(filenames, base):\n",
    "    \"\"\"\n",
    "    loads a set of tensorboard logs into scalar dataframes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for fn in filenames:\n",
    "        print(f\" - reading {fn}\")\n",
    "        reader = get_logs(fn, extra_columns={'wall_time', }, base=base)\n",
    "        sc = parse_scalars(reader)\n",
    "        # print(f\"loaded {sc.shape[0]} rows from {fn}\")\n",
    "        data.append(sc)\n",
    "    return data\n",
    "\n",
    "def merge_data(dataframes, add_date_column=False, keep_only_eval_rows=True):\n",
    "    print(\" - merging dataframes\")\n",
    "    df_all = pd.concat(dataframes).reset_index(drop=True)\n",
    "    df_all['wall_time'] = df_all['wall_time'].astype('int')\n",
    "    if add_date_column:\n",
    "        df_all['date'] = pd.to_datetime(df_all['wall_time'], unit='s').astype('datetime64[s]')\n",
    "    if \"fps\" in df_all.columns:\n",
    "        df_all['fps'] = df_all['fps'].ffill()\n",
    "    if keep_only_eval_rows:\n",
    "        df_all = df_all[df_all['eval_episode_rewards'].notna()]\n",
    "    return df_all\n",
    "\n",
    "def combine_tensorlogs(model, cfg, base_path='../tensorboard_log/cs', target_path='../logs_parsed', \n",
    "                       add_date=False, keep_only_eval_rows=True):\n",
    "    print(f\"Combine Logs for {model}_{cfg}\")\n",
    "    start = time.time()\n",
    "    filenames = get_filenames(model, cfg, base=base_path)\n",
    "    \n",
    "    if len(filenames) == 0:\n",
    "        print(f\"No files found - skipping\")\n",
    "        return\n",
    "    print(f\"{len(filenames)} files found\")\n",
    "    \n",
    "    x = load_dataframes(filenames, base=base_path)\n",
    "    x = fix_counters(x)\n",
    "    x = merge_data(x, add_date_column=add_date, keep_only_eval_rows=keep_only_eval_rows)\n",
    "\n",
    "    x.to_csv(f\"{target_path}/{model}_{cfg}.csv\")\n",
    "    print(f\"Combine Logs for {model}_{cfg} done in {(time.time() - start):.1f}s\")\n",
    "        \n",
    "    return x\n",
    "\n",
    "def combine_tensorlogs_intraday(model, cfg, res, base_path='../tensorboard_log/cs', target_path='../logs_parsed', \n",
    "                       add_date=False, keep_only_eval_rows=True):\n",
    "    print(f\"Combine Logs for {model}_{cfg}_{res}\")\n",
    "    start = time.time()\n",
    "    filenames = get_filenames(model, cfg, base=base_path, res=res)\n",
    "    \n",
    "    if len(filenames) == 0:\n",
    "        print(f\"No files found - skipping\")\n",
    "        return\n",
    "    print(f\"{len(filenames)} files found\")\n",
    "    \n",
    "    x = load_dataframes(filenames, base=base_path)\n",
    "    x = fix_counters(x)\n",
    "    x = merge_data(x, add_date_column=add_date, keep_only_eval_rows=keep_only_eval_rows)\n",
    "\n",
    "    x.to_csv(f\"{target_path}/{model}_{cfg}_{res}.csv\")\n",
    "    print(f\"Combine Logs for {model}_{cfg} done in {(time.time() - start):.1f}s\")\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fe94b-7f26-4452-adc3-71e6a11741b9",
   "metadata": {},
   "source": [
    "# A2C Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919d12c-2f07-4df4-a160-30b4f481b7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfgs = ['V213', 'V219', 'V221']\n",
    "for cfg in cfgs:\n",
    "    df = combine_tensorlogs(\"A2C\", cfg, add_date=True, base_path='../tensorboard_log/_old_A2C_v2xx', keep_only_eval_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0ae99-a35f-4e2b-97ba-1f7df03a8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = ['V221']\n",
    "reslist = [\"1H\", \"6H\", \"12H\"]\n",
    "for cfg in cfgs:\n",
    "    for res in reslist:\n",
    "        df = combine_tensorlogs_intraday(\"A2C\", cfg, res, add_date=True, base_path='../tensorboard_log/intraday_A2C', keep_only_eval_rows=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50836197-768f-4c9d-a147-75c5f0216686",
   "metadata": {},
   "source": [
    "# PPO Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71d6e2-7915-4371-8931-8fae22fb60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = ['V205', 'V207', 'V208']\n",
    "for cfg in cfgs:\n",
    "    df = combine_tensorlogs(\"PPO\", cfg, add_date=True, base_path='../tensorboard_log/_old_PPO_v2xx', keep_only_eval_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9909fd-c225-43db-85b0-884aa95da53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = ['V208']\n",
    "reslist = [\"1H\", \"6H\", \"12H\"]\n",
    "for cfg in cfgs:\n",
    "    for res in reslist:\n",
    "        df = combine_tensorlogs_intraday(\"PPO\", cfg, res, add_date=True, base_path='../tensorboard_log/intraday_PPO', keep_only_eval_rows=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e4122-b83b-4dbc-bc67-4ef02ce59d82",
   "metadata": {},
   "source": [
    "# TD3 Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc72483-e97c-44c1-b045-550cc9499c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = ['V203', 'V206', 'V213', 'V214']\n",
    "for cfg in cfgs:\n",
    "    df = combine_tensorlogs(\"TD3\", cfg, add_date=True, base_path='../tensorboard_log/_old_TD3_v2xx', keep_only_eval_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3520e38-762d-4aa4-83ac-f3212a7e14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = ['V214']\n",
    "reslist = [\"1H\", \"6H\", \"12H\"]\n",
    "for cfg in cfgs:\n",
    "    for res in reslist:\n",
    "        df = combine_tensorlogs_intraday(\"TD3\", cfg, res, add_date=True, base_path='../tensorboard_log/intraday_TD3', keep_only_eval_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e91174-3e3d-4863-9c4f-c30cdb7eba7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
